{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler, random_split\n",
    "import timeit\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from plot import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  Parameters  ################\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "SEPCTURAL_SAMPLES = 10\n",
    "FEATURE_DIM = SEPCTURAL_SAMPLES*6*2\n",
    "CONV_LEN = 3\n",
    "CONV_LEN_INTE = 3#4\n",
    "CONV_LEN_LAST = 3#5\n",
    "CONV_NUM = 64 #CONV_NUM = out channels = in channels * conv_num'\n",
    "CONV_MERGE_LEN = 8\n",
    "CONV_MERGE_LEN2 = 6\n",
    "CONV_MERGE_LEN3 = 4\n",
    "CONV_NUM2 = 64\n",
    "INTER_DIM = 120\n",
    "OUT_DIM = 6#len(idDict)\n",
    "WIDE = 20\n",
    "CONV_KEEP_PROB = 0.8\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_ITER_NUM = 30000\n",
    "\n",
    "select = 'a'\n",
    "\n",
    "metaDict = {'a':[119080, 1193], 'b':[116870, 1413], 'c':[116020, 1477]}\n",
    "TRAIN_SIZE = metaDict[select][0]\n",
    "EVAL_DATA_SIZE = metaDict[select][1]\n",
    "EVAL_ITER_NUM = int(math.ceil(EVAL_DATA_SIZE / BATCH_SIZE))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "defaultVal = [[0.] for idx in range(2*3 + 1)]\n",
    "print(defaultVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data of the subject and returns df with the relevant information\n",
    "def prep_data(path, subject):\n",
    "    data = pd.read_csv(path, sep=' ', header=None)\n",
    "    data.columns = pd.RangeIndex(1, len(data.columns) + 1) \n",
    "    data.drop(3, axis='columns', inplace=True)\n",
    "    data = data.dropna()\n",
    "    data = data[[1,2,22,23,24,28,29,30]]\n",
    "    cols = {1: 'time_step', 2: 'activity_id', 22: 'acc_x', 23: 'acc_y', 24: 'acc_z', 28: 'gyro_x', 29: 'gyro_y', 30: 'gyro_z'}\n",
    "    data = data.rename(columns=cols)\n",
    "    # calculating norm\n",
    "    data['acc_norm'] = np.sqrt(data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2)\n",
    "    data['gyro_norm'] = np.sqrt(data['gyro_x'] ** 2 + data['gyro_y'] ** 2 + data['gyro_z'] ** 2)\n",
    "    data['User'] = f'{subject}'\n",
    "    print(f'{subject}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/Users/momo/Documents/dataset/pamap2+physical+activity+monitoring/Protocol/subject101.dat', sep=' ', header=None)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# Perform preprocess for different subjects\n",
    "# subjects = ['101', '102', '105', '106', '107', '108']\n",
    "subjects = ['101','102']\n",
    "path = '/Users/momo/Documents/dataset/pamap2+physical+activity+monitoring/Protocol'\n",
    "data_list = []\n",
    "for subject in subjects:\n",
    "    data_list.append(prep_data(path + '/subject' + subject + '.dat', subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   time_step  activity_id     acc_x    acc_y    acc_z    gyro_x    gyro_y  \\\n",
       " 0       8.38            0  0.238080  9.80003 -1.68896 -0.005065 -0.006781   \n",
       " 1       8.39            0  0.319530  9.61282 -1.49328  0.013685  0.001486   \n",
       " 2       8.40            0  0.235593  9.72421 -1.76621 -0.039923  0.034056   \n",
       " 3       8.41            0  0.388697  9.53572 -1.72410  0.007513 -0.010498   \n",
       " 4       8.42            0  0.315800  9.49908 -1.60914 -0.003822 -0.011217   \n",
       " \n",
       "      gyro_z  acc_norm  gyro_norm User  \n",
       " 0 -0.005663  9.947354   0.010184  101  \n",
       " 1 -0.041522  9.733360   0.043744  101  \n",
       " 2 -0.002113  9.886115   0.052518  101  \n",
       " 3 -0.020684  9.698122   0.024382  101  \n",
       " 4 -0.025975  9.639584   0.028550  101  ,\n",
       "    time_step  activity_id    acc_x    acc_y    acc_z    gyro_x    gyro_y  \\\n",
       " 0       5.64            0  1.94739  9.59644 -3.12873  0.124025  0.112482   \n",
       " 1       5.65            0  1.75120  9.63340 -3.32601  0.132679  0.060829   \n",
       " 2       5.66            0  1.67059  9.70790 -3.48260  0.074772  0.124062   \n",
       " 3       5.67            0  1.66925  9.63234 -3.52110  0.063729  0.136592   \n",
       " 4       5.68            0  1.58969  9.66945 -3.63882  0.008942  0.107000   \n",
       " \n",
       "      gyro_z   acc_norm  gyro_norm User  \n",
       " 0 -0.044947  10.279734   0.173363  102  \n",
       " 1 -0.044168  10.340766   0.152495  102  \n",
       " 2 -0.053608  10.448095   0.154454  102  \n",
       " 3  0.004851  10.390694   0.150806  102  \n",
       " 4  0.003266  10.453056   0.107423  102  )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].head(),data_list[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to one df\n",
    "for d in range(len(data_list)):\n",
    "  if d==0:\n",
    "    df = data_list[0]\n",
    "  else:\n",
    "    df = pd.concat([df, data_list[d]], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, cols): #list of columns\n",
    "    df_t=(df[cols]-df[cols].mean())/df[cols].std() #均值为0，方差为1的分布\n",
    "    df_norm = df.copy()\n",
    "    df_norm[cols] = df_t\n",
    "\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = normalize(df, ['acc_norm', 'gyro_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, cols, sigma=0.1):\n",
    "    noise = np.random.normal(0, sigma, df[cols].shape)\n",
    "    new_signal = df[cols] + noise\n",
    "    df_noise = df.copy()\n",
    "    df_noise[cols] = new_signal\n",
    "\n",
    "    return df_noise\n",
    "\n",
    "def augment(df, cols, num_of_inst):\n",
    "    '''\n",
    "    df should be inserted normalized\n",
    "    num_of_inst = num of instances to create from each instance\n",
    "    '''\n",
    "    aug_df = df.copy()\n",
    "    for i in range(num_of_inst):\n",
    "        np.random.seed(i)\n",
    "        nois_data = add_noise(df, cols)\n",
    "        nois_data['User'] = nois_data['User'] + f'{i}'\n",
    "        aug_df= pd.concat([aug_df, nois_data], axis=0, ignore_index=True)\n",
    "\n",
    "    return aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = augment(data_norm, ['acc_norm', 'gyro_norm'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_step</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>acc_norm</th>\n",
       "      <th>gyro_norm</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.238080</td>\n",
       "      <td>9.80003</td>\n",
       "      <td>-1.68896</td>\n",
       "      <td>-0.005065</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>-0.066101</td>\n",
       "      <td>-0.818660</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.319530</td>\n",
       "      <td>9.61282</td>\n",
       "      <td>-1.49328</td>\n",
       "      <td>0.013685</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>-0.041522</td>\n",
       "      <td>-0.128133</td>\n",
       "      <td>-0.764882</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235593</td>\n",
       "      <td>9.72421</td>\n",
       "      <td>-1.76621</td>\n",
       "      <td>-0.039923</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>-0.083853</td>\n",
       "      <td>-0.750823</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388697</td>\n",
       "      <td>9.53572</td>\n",
       "      <td>-1.72410</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>-0.010498</td>\n",
       "      <td>-0.020684</td>\n",
       "      <td>-0.138347</td>\n",
       "      <td>-0.795908</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>9.49908</td>\n",
       "      <td>-1.60914</td>\n",
       "      <td>-0.003822</td>\n",
       "      <td>-0.011217</td>\n",
       "      <td>-0.025975</td>\n",
       "      <td>-0.155316</td>\n",
       "      <td>-0.789230</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_step  activity_id     acc_x    acc_y    acc_z    gyro_x    gyro_y  \\\n",
       "0       8.38            0  0.238080  9.80003 -1.68896 -0.005065 -0.006781   \n",
       "1       8.39            0  0.319530  9.61282 -1.49328  0.013685  0.001486   \n",
       "2       8.40            0  0.235593  9.72421 -1.76621 -0.039923  0.034056   \n",
       "3       8.41            0  0.388697  9.53572 -1.72410  0.007513 -0.010498   \n",
       "4       8.42            0  0.315800  9.49908 -1.60914 -0.003822 -0.011217   \n",
       "\n",
       "     gyro_z  acc_norm  gyro_norm User  \n",
       "0 -0.005663 -0.066101  -0.818660  101  \n",
       "1 -0.041522 -0.128133  -0.764882  101  \n",
       "2 -0.002113 -0.083853  -0.750823  101  \n",
       "3 -0.020684 -0.138347  -0.795908  101  \n",
       "4 -0.025975 -0.155316  -0.789230  101  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aug.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['101', '1010', '102', '1020'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = data_aug\n",
    "np.unique(aaa['activity_id'])\n",
    "np.unique(aaa['User'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>timstep</th>\n",
       "      <th>user</th>\n",
       "      <th>acc_spec</th>\n",
       "      <th>gyro_spec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [activity, timstep, user, acc_spec, gyro_spec]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data = data_aug\n",
    "unique_actions = np.unique(initial_data['activity_id'])\n",
    "unique_users = np.unique(initial_data['User'])\n",
    "dff = pd.DataFrame(columns=['activity','timstep', 'user','acc_spec','gyro_spec'])\n",
    "print(dff.shape)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n"
     ]
    }
   ],
   "source": [
    "print(unique_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/momo/Desktop'\n",
    "for action in [1]:\n",
    "    print('action: ', action)\n",
    "    for user in ['101']:\n",
    "        print('user: ', user)\n",
    "        act_user_temp = initial_data[(initial_data['activity_id']==action) & (initial_data['User']==user)]\n",
    "        NFFT = 25\n",
    "        noverlap = int(0.25 * NFFT)\n",
    "        print(act_user_temp.shape, noverlap)\n",
    "        fig = plt.figure(frameon=False)\n",
    "        sr = 100\n",
    "        timestep = 0\n",
    "        for row in range(0, act_user_temp.shape[0], 150): # overlap of the data is 50%. not the hyperparameter of FFT.\n",
    "            timestep += 1\n",
    "            spec_acc, freqenciesFound_x, time, imageAxis_x = plt.specgram(act_user_temp['acc_norm'][row:row + 200], Fs=100, NFFT=NFFT, noverlap=noverlap, window=np.hamming(NFFT),cmap='viridis')\n",
    "            spec_gyro, freqenciesFound_x, time, imageAxis_x = plt.specgram(act_user_temp['gyro_norm'][row:row + 200], Fs=100, NFFT=NFFT, noverlap=noverlap, window=np.hamming(NFFT),cmap='viridis')\n",
    "            instance = pd.DataFrame({'activity':action, 'timstep':timestep, 'user':user, 'acc_spec':[spec_acc], 'gyro_spec':[spec_gyro]})\n",
    "            dff = dff.append(instance, ignore_index=True) # label, timestep, user, f*t (spectrogram), f*t (spectrogram)\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27179, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_user_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_user_temp = initial_data[(initial_data['activity_id']==action) & (initial_data['User']==user)]\n",
    "NFFT = 25\n",
    "noverlap = int(0.25 * NFFT)\n",
    "print(act_user_temp.shape, noverlap)\n",
    "fig = plt.figure(frameon=False)\n",
    "sr = 100\n",
    "timestep = 0\n",
    "for row in range(0, act_user_temp.shape[0], 150): # overlap of the data is 50%. not the hyperparameter of FFT.\n",
    "    timestep += 1\n",
    "    spec_acc, freqenciesFound_x, time, imageAxis_x = plt.specgram(act_user_temp['acc_norm'][row:row + 200], Fs=100, NFFT=NFFT, noverlap=noverlap, window=np.hamming(NFFT),cmap='viridis')\n",
    "    spec_gyro, freqenciesFound_x, time, imageAxis_x = plt.specgram(act_user_temp['gyro_norm'][row:row + 200], Fs=100, NFFT=NFFT, noverlap=noverlap, window=np.hamming(NFFT),cmap='viridis')\n",
    "    instance = pd.DataFrame({'activity':action, 'timstep':timestep, 'user':user, 'acc_spec':[spec_acc], 'gyro_spec':[spec_gyro]})\n",
    "    dff = dff.append(instance, ignore_index=True) # label, timestep, user, f*t (spectrogram), f*t (spectrogram)\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dff = []\n",
    "for row in range(0,1000,150):\n",
    "    spec_acc, freqenciesFound_x, time, imageAxis_x = plt.specgram(S[row:row + 200], Fs=100, NFFT=NFFT, noverlap=noverlap, window=np.hamming(NFFT),cmap='viridis')\n",
    "    dff.append(spec_acc)\n",
    "    plt.imshow(spec_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 10)\n",
      "(13, 10)\n",
      "(13, 10)\n",
      "(13, 10)\n",
      "(13, 10)\n",
      "(13, 10)\n",
      "(13, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in dff:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5x/3g67fwns0mb68s__c_3fk4wr0000gn/T/ipykernel_54772/4058345429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "dff = torch.tensor(dff[:-1])\n",
    "dff = torch.cat(dff[:-1], dim=0)\n",
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc05ae8f3d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAD6CAYAAADwU1ScAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALHUlEQVR4nO2dbYwdVRnHf/+7r91t6XbbUtpSLCJUAbVArfJiUl9QUo0oiS81IZKYIB9INOGDjSaK30gUJUZjggEBgyBGQUIoWoiEgFQoDRSwpS1NhW1LS3e7293tdnfv3scPd7Zc9szdO3dm77Q7c37Jzb33mTMz579n7rMz5znPOTIz8kDhVFcgLbzQrOGFZg0vNAqSrpH0hqQ9kjbOVKUageL+H5XUBOwCrgZ6gBeBDWb232r7zF3Qat3L2x37wO45YWdwLCPFAcYmRtwNEWiOs1PAWmCPme0FkPQgcC1QVWj38nZu+ctax/7E+o+5heXq+ff++2NXNsmluxx4u+J7T2A7LUkiNOwScn4Hkm6UtFXS1qG+8QSnS0YSoT3AiorvZwMHphYyszvNbI2ZrZnb3ZLgdMlI8ht9EThf0rnAfuBbwLen22Fooo3n+s9zN4T8HimEtEEsN1QmtlAzK0q6GfgH0ATcbWavx69KY0nSopjZ48DjM1SXhuLvjLJGboQm+o3WS1uhyIc633Xs24Zcd6o5IbeFCbp9ctOiXmjWyI3QVJ3RsfF2njywyrEvnDvm2KzDfW7lcPx2yU2LeqFZwwvNGql63YKMOS0h3SkhD94aGHLLTZTinzv2nrMMLzRrJPqNStoHDAITQNHM1sxEpRrBTDijz5jZkSgFhdFcCHEo4yG25qawA8QmN5duUqEG/FPSS5JunIkKNYqkl+6VZnZA0pnAZkk7zeyZygLBH+BGgLYl8xKeLj6JWtTMDgTvh4GHKUfYppY5GZJonR8WHkyH2EIldUqaN/kZ+ALw2kxVbKZJcukuAR5W+fatGfiTmT0x3Q4thQmWdgw49iMhvYAs6nZtYTGaiCSJvewFPh77zCnj/71kjdwITfV5tKNpnNXzehz7U10XOzbr7XcPUJyIfe7ctKgXmjW80KyRrtctjLKmY69jf7J/hVu4OaRq/sG7Nl5o1siN0FSd0YlSCztHl7kbFi90bX39M3ru3LSoF5o1ciO0pjOSdDfwZeCwmV0c2LqBPwMrgX3AN8zsaK1jFa2JI+Nu367G3JiphXWODcX3nVFa9B7gmim2jcBTZnY+8FTw/bSmptCg571vivla4N7g873AV2e2WjNP3N/oEjM7CBC8n1mtYGWWxPBRd+BUWjTcGVWGJDoXtDb6dFWJK/SQpKUAwfvhmatSY4jrxh4FvgPcFrz/PcpO3U3DbJj/kmN/5p0ux9bUvcA9wEQDewElPQA8D6yS1CPpu5QFXi1pN+UkvNti1yAlaraomW2osulzM1yXhpKbO6PcCE31efS4tbIt5Hm0sNLtHCsdOOQewA+Rq40XmjW80KyRqtft1BifbHeyo7nryNSnQFBbyAPA8fgxidy0qBeaNXIjNFVnNGLNvDK2yLGr3c1Dm1jmhilsOH4PRW5a1AvNGl7oJJLulnRY0msVtlsl7Zf0cvBa39hqJieK170H+A1w3xT7r8zsF/WcrH+ig0f7LnE3tLjVaOoddGxqZC9glZDErCPJb/RmSduDSzukE/b0Iq7Q3wHnAauBg8Dt1QpWxl5OHB2NebrkxBJqZofMbMLMSsDvCUkDqSh7MvbSvqAtbj0TE+sWUNLSyWga8DUipoF0Fka5/Iw3HftDw3PdwoWQZ88EvYBRIt4PAOuARZJ6gJ8C6yStppyytQ/4XuwapETckMRdDahLQ/F3RlkjN0JTffDuL3bwyCH3FtBOHHdsWhoyLOJoSJZwRHLTol5o1siN0FSd0VmtA2xc4c6AeevYFY6tMDjsHqDkc9Nq4oVmDS80a6TqdQW0yPWcYcNvJva97djMZwTXxgvNGlFCEisk/UvSDkmvS/p+YO+WtFnS7uD9tO7bjeKMisAtZrYtmATmJUmbgRsoZ0rcFqwMshH44XQH6pS4LGy0Sf8xx1SY5/YMqtjASUrN7KCZbQs+DwI7KK8ZMasyJer6E0laCVwC/Ic6MiVOByILlTQX+CvwAzNzr7Xq+50MSbzbG///YFIiCZXUQlnk/Wb2t8AcKVOiMiSxeGH8Pp+kROmpF+UO6x1m9suKTXVnSuwc6eKq7dc59s5DbiZ/89KzwipT6xRVieJ1rwSuB16V9HJg+xFlgQ8FWRNvAV+PXYsUiBKSeJbqMyDMmkwJf2eUNXIjNNXn0e7W43xzhZub9sSKj7qFS2FBXz8wuSZeaNbIjdB0syQKo1zRsduxb+r4hGPTsZDp292F9iKTmxb1QrOGF5o1UvW6vcW5/OHIpx17KST8UOia7x4gvtPNT4t6oVkjSUhiVmVKJAlJQJ2ZEl1Nx/nKgm2O/fbxkOWti8WQI8T3RlE6xw5SHjePmQ1KmgxJzCqShCRgFmVKJAlJRMqUqAxJDPSFXY7pEDskETVTojIkMb871fuT9xHF64aGJCbjLgGRMyVOFUlCEhvqzZQYKrXx3PAFbiXOcX1b8Uz3FtBei58RnCQkMSvW757E3xlljdwITdXfF62J3jF3tEmp153fdPwCd0iENfmQRE280KzhhWaNVL1uycRoyT2lfXilY2vfssuxFYZPxD53blrUC80auRGaqjOa0zTORzoPOvb9QyELaYRNT+BHpdTGC80aUTrH2iW9IOmVICTxs8CeuSyJUeCzZjYUdHs+K2kTcB11ZkmMTLTw6lBIJ/877jLgmn+GW264sVkSZmaTY2FagpeRxSwJSU1BV+dhYLOZZTNLIuiRXw2cDayV5C4YWoX3TdzUH/+mPCl1XfRm1g88TXn9l7qzJNq73Kkr0yKK110sqSv4PAf4PLCT97IkoI71JE4VUbzuUuBeSU2U/zAPmdljkp6nziyJohXoHe10N9iIYyp1hcxadSh+3kyUkMR2yjHRqfZefJbE6YcXmjVSXsd7jMu63nLsW85xR6XYTjdfzUbjT6CYmxb1QrOGF5o1UvW6bSpybpt77/9Cnzs7soXMn6J9fp76mnihWSM3QtNdN63UyrbhlY7d5nU4Ng2683n6ddMi4IVmjSQhicxlSVQLSUCM9SROFVE6xwwIC0nUTXthnAs73HXTdo+4geDiMnfBcutr7ILl1UISkLUsiSohibqzJIaOukvNp0XskEScLIm5C1qS1jc2sUMSWcySqBaS+GO9WRKDE+083bfK3TDqLk1f2LHPsWkkfi9gkpDE9bHPegrwd0ZZwwvNGqmn/5VCxvOVhtw56f3SuTHxQrNGboSm6owWNg9xw5LnHPsdY6sdm5YtcQ8w2ODn0SzghWYNLzRrpOp13xmbz8/3fdGxt60IGePXN+Da/KIZtfFCs0ZuhKoccUjpZNK7wP+Cr4sANw9kelaZ2bw4507V65rZ4snPkraa2Zp69pe0Ne65c3PpeqEpcGdK+wApO6NTib90ZwJJ10h6Q9KeICNx6nZJ+nWwfbukS6vNLDllv3WSBirGT/ykZmXMrCEvoAl4E/gg0Aq8Alw4pcx6YBPl5O1PUZ5vcClwabB9HrArZL91wGP11KeRLboW2GNme81sDHiQcipmJdcC9wWpm1uALoAqi10lopFClwOVMwT34FZ42jIhM0tWcnkwJGiTpItqVaaRd0Zh8YOpLr5qmRqLXW0DPhAMCVoPPAKcP11lGtmiPUDlMOqzgaljb0LLVFns6iRmdmwyS9nMHgdaJC2atjYNdEbNwF7gXN5zRhdNKfMl3u+MXgg+3wfcMc2xz+K9e4C1lLMdNW19GiW0wqvuoux9fxzYbgJuCj4L+G2w/VVgDXAV5ct3O/By8Fo/Zb+bgdeDP94W4IpadfF3RlnDC80aXmjW8EKzRm6E/h8H9DecFeB+2wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(spectrogram[:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_pickle(path+\"/labeled_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(path+\"/labeled_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 filtering targeted activities and making the labels sequencials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['activity'].isin([1,2,3,4,5,6,7,12,13,16,17])]\n",
    "df['activity'].loc[(df['activity']==12)] = 8\n",
    "df['activity'].loc[(df['activity']==13)] = 9\n",
    "df['activity'].loc[(df['activity']==16)] = 10\n",
    "df['activity'].loc[(df['activity']==17)] = 0\n",
    "df[df['activity'] == 10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 =  df[df['activity']==0]\n",
    "df0_100 = df0.reset_index().iloc[:1000]\n",
    "\n",
    "df1 =  df[df['activity']==1]\n",
    "df1_100 = df1.reset_index().iloc[:1000]\n",
    "\n",
    "df2 =  df[df['activity']==2]\n",
    "df2_100 = df2.reset_index().iloc[:1000]\n",
    "\n",
    "df3 =  df[df['activity']==3]\n",
    "df3_100 = df3.reset_index().iloc[:1000]\n",
    "\n",
    "df4 =  df[df['activity']==4]\n",
    "df4_100 = df4.reset_index().iloc[:1000]\n",
    "\n",
    "df5 =  df[df['activity']==5]\n",
    "df5_100 = df5.reset_index().iloc[:1000]\n",
    "\n",
    "df6 =  df[df['activity']==6]\n",
    "df6_100 = df6.reset_index().iloc[:1000]\n",
    "\n",
    "df7 =  df[df['activity']==7]\n",
    "df7_100 = df7.reset_index().iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df0_100, df1_100, df2_100, df3_100,df4_100,df5_100,df6_100,df7_100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_df = pd.DataFrame(columns=['id', 'input_5_timesteps', 'label'])\n",
    "id = 0\n",
    "for user in unique_users:\n",
    "    for action in unique_actions:\n",
    "        ua_df = df[(df['user']==user) & (df['activity']==action)]\n",
    "        for t in range(0, ua_df.shape[0], 5):\n",
    "          vec_acc = ua_df.iloc[t:t+5]['acc_spec'].tolist()\n",
    "          vec_gyro = ua_df.iloc[t:t+5]['gyro_spec'].tolist()\n",
    "          vec_acc_gyro = [vec_acc] + [vec_gyro]\n",
    "          instance = pd.DataFrame({'id': id, 'input_5_timesteps': [vec_acc_gyro], 'label': action})\n",
    "          loader_df = loader_df.append(instance, ignore_index=True)\n",
    "          id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/3g67fwns0mb68s__c_3fk4wr0000gn/T/ipykernel_54772/607291761.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1639180852547/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x_tensor = torch.FloatTensor(x[i])\n"
     ]
    }
   ],
   "source": [
    "x = loader_df['input_5_timesteps']\n",
    "id = []\n",
    "label = []\n",
    "tensor_input = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "  try:\n",
    "    x_tensor = torch.FloatTensor(x[i])\n",
    "    # print(x_tensor.shape)\n",
    "    if x_tensor.shape == torch.Size([2, 5, 13, 10]):\n",
    "      tensor_input.append(x_tensor)\n",
    "      label.append(loader_df['label'][i])\n",
    "      id.append(loader_df['id'][i])\n",
    "\n",
    "  except:\n",
    "    print('*********',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(label, bins=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 36)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id), len(label), len(tensor_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([36]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = torch.tensor(id)\n",
    "label = torch.tensor(label)\n",
    "tensor_input = torch.stack(tensor_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 2, 5, 13, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_input.shape # (num_of_instances, K:2, t:5, feature: 13,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 training samples\n",
      "8 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(id, tensor_input, label)\n",
    "train_size = int(0.80 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'{train_size} training samples')\n",
    "print(f'{val_size} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "            )\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5, 13, 10])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader,0):\n",
    "    _, inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 5, sharex=True, sharey=False, figsize=(15,5))\n",
    "instance = next(iter(train_dataloader))\n",
    "for i in range(5):\n",
    "    axes[0,i].imshow(instance[1][0][0][i])\n",
    "    axes[0,i].set_title(f'acc timestep {i+1}')\n",
    "for i in range(5):\n",
    "    axes[1,i].imshow(instance[1][0][1][i])\n",
    "    axes[1,i].set_title(f'gyro timestep {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data.shape: [B, K, T, F]\n",
    "====== train CNN for every mode ======\n",
    "conv_in.shape: [B, C, T, F]\n",
    "conv_out.shape:[B, T, D] D=C*F\n",
    "============ merge modes =============\n",
    "attn1_in.shape: [B, T, M, D] M:mode\n",
    "attn1_out.shape:[B, T, D]\n",
    "============ train GRU ===============\n",
    "gru_in.shape: [B, T, D]\n",
    "gru_out.shape:[B, T, D']\n",
    "=========== merge time info ==========\n",
    "attn2_in.shape: [B, T, D']\n",
    "attn2_out.shape:[B, D']\n",
    "fc_in.shape: [B, N] N: num_activities\n",
    "output.shape:[B, N]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    # input shape: Batch,in_channels,H(height),W(width)\n",
    "    # in this case: Batch, in_channels, time_window, frequency\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding='valid'):\n",
    "        super(Conv, self).__init__()\n",
    "        if padding == 'valid':\n",
    "            self.padding = 0\n",
    "        elif padding == 'same':\n",
    "            self.padding = 'same'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding mode. Use 'valid' or 'same'.\")\n",
    "            \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels[0], kernel_size[0], stride=(1, 2*3), padding=self.padding),\n",
    "            nn.BatchNorm2d(out_channels[0]), \n",
    "            # Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=1-CONV_KEEP_PROB),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels[0], out_channels[1], kernel_size[1], stride=(1,1), padding=self.padding),\n",
    "            nn.BatchNorm2d(out_channels[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=1-CONV_KEEP_PROB),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels[1], out_channels[2], kernel_size[2], stride=(1,1), padding=self.padding),\n",
    "            nn.BatchNorm2d(out_channels[2]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # self.fc = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print('x.shape 1: ', inputs.shape)\n",
    "        x = self.conv1(inputs)\n",
    "        print('x.shape 2: ', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print('x.shape 3: ', x.shape)\n",
    "        x = self.conv3(x)\n",
    "        print('x.shape 4: ', x.shape)\n",
    "        x = x.unsqueeze(2) # shape = [B,C,1,f,t]\n",
    "        # x = torch.flatten(x, start_dim=1) # shape = [B,C*f*t]\n",
    "        # print('x.shape 5: ', x.shape)\n",
    "        # x = self.fc(x)\n",
    "        # print('x.shape 6: ', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mode(inputs): # Attention-fusion Subnet\n",
    "    \"\"\"\n",
    "    input shape: [batch, time_window, input_mode, feature_dim(f*channels)]\n",
    "    output shape:[batch, time_window, feature_dim]\n",
    "    \n",
    "    \"\"\"\n",
    "    d = inputs.shape[-1]\n",
    "\n",
    "    w = torch.nn.Parameter(torch.randn(d, requires_grad=True) * 0.1) # weights\n",
    "    b = torch.nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "\n",
    "    activation = torch.tanh(torch.tensordot(inputs, w, dims=1) + b)  # b * t * m\n",
    "    alphas = F.softmax(activation, dim=2)  # b * t * m\n",
    "    print(\"alphas \", str(alphas.shape))\n",
    "\n",
    "    # l = inputs.shape[-2]\n",
    "    # print(d,l)\n",
    "    # w = torch.randn([d,l], requires_grad=True, dtype=torch.double) * 0.1 # weights\n",
    "    # b = torch.randn(l, requires_grad=True, dtype=torch.double) * 0.1 \n",
    "    # omega = torch.randn(l, requires_grad=True, dtype=torch.double) * 0.1 \n",
    "    # activation = torch.tanh(torch.matmul(inputs,w) + b)  # b * t * m * m = [b,t,i,d] * [d,i]\n",
    "    # activation = torch.matmul(activation,omega) # b * t * m = [b,t,i,i] * [i]\n",
    "    # alphas = F.softmax(activation, dim=2)  # b * t * m\n",
    "    # print(\"alphas \", str(alphas.shape))\n",
    "    \n",
    "    beta = inputs * alphas.unsqueeze(-1)\n",
    "    print(beta.shape)\n",
    "    output = torch.sum(beta, dim=2)  # b * t * d = [b,t,i,d] * [b,t,i,1] element_wise\n",
    "    print(output.shape) #shape: [batch, time_window, feature_dim]\n",
    "    return alphas, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_time(inputs, attention_size):\n",
    "    \"\"\"\n",
    "    input shape: [batch, time_window, feature_dim]\n",
    "    output shape:[batch, feature_dim]\n",
    "    \"\"\"\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = torch.cat(inputs, 2)\n",
    "    \n",
    "    hidden_size = inputs.shape[2]  #inputs: (B,T,D)\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = nn.Parameter(torch.randn(hidden_size, attention_size) * 0.1)\n",
    "    b_omega = nn.Parameter(torch.randn(attention_size) * 0.1)\n",
    "    u_omega = nn.Parameter(torch.randn(attention_size) * 0.1)\n",
    "    # with torch.name_scope('v'):\n",
    "    #     # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "    #     # the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "    #     v = torch.tanh(torch.matmul(inputs, w_omega) + b_omega)\n",
    "\n",
    "    v = torch.tanh(torch.matmul(inputs, w_omega) + b_omega)  # (B,T,A) shape\n",
    "    # For each of the timestamps, its vector of size A from `v` is reduced with the `u` vector\n",
    "    vu = torch.matmul(v, u_omega).squeeze(-1)  # (B,T) shape\n",
    "    alphas = F.softmax(vu, dim=1)   # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with the attention vector; the result has (B,D) shape\n",
    "    output = torch.sum(inputs * alphas.unsqueeze(-1), dim=1)\n",
    "\n",
    "    return output, alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 AttnSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnSense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttnSense, self).__init__()\n",
    "        self.attn1 = attention_mode()\n",
    "        self.attn2 = attention_time()\n",
    "        self.conv = Conv()\n",
    "\n",
    "        # Define GRU layer\n",
    "        # nn.GRU(input_size, hidden_size, num_layers)\n",
    "        # input:[batch, seq_len, input_size], h_0:[num_layers * num_directions, batch, hidden_size], defaults to zeros if not provided.\n",
    "        # output: [batch, seq_len, hidden_size * num_directions], containing the output features (h_t) from the last layer of the GRU, for each t.\n",
    "        # h_n:[num_layers * num_directions, batch, hidden_size], containing the final hidden state for the input sequence.\n",
    "        self.gru1 = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc_gru = nn.Linear(hidden_size, 8) # input: [B,D]\n",
    "\n",
    "        self.AZ = 80\n",
    "        self.attn2_out = None\n",
    "\n",
    "    def forward(self, inputs, modes):\n",
    "        # split data based on modality and train Conv for every modality.\n",
    "        m = []\n",
    "        for i in range(modes):\n",
    "            m.append(inputs[0][i].reshape(B,T,K,D))\n",
    "\n",
    "        conv_out_1 = self.conv(m[0], \"acc\", train=self.train)\n",
    "        conv_out_2 = self.conv(m[1], \"gyro\", train=self.train)\n",
    "        conv_out_all = torch.cat([conv_out_1, conv_out_2], dim=2)\n",
    "        print('conv_out.shape:', conv_out_1.shape, conv_out_2.shape, conv_out_all.shape)\n",
    "\n",
    "        attn1_in = conv_out_all.view(conv_out_all.size(0), conv_out_all.size(1), conv_out_all.size(2),\n",
    "                                            conv_out_all.size(3) * conv_out_all.size(4))\n",
    "        print(attn1_in.shape)\n",
    "        attn1_out = self.attn1(attn1_in)\n",
    "\n",
    "        # self.init_state = torch.zeros(2, attn1_out.size(0), self.cell.hidden_size, dtype=torch.float32)\n",
    "        # h_t = self.init_state\n",
    "        # for t in range(attn1_out.size(1)):\n",
    "        #     h_t = self.cell(attn1_out[:, t], h_t)\n",
    "        #     gru_out.append(h_t)\n",
    "        # gru_out = torch.stack(gru_out, dim=1)  # (batch, time_window, DIM)\n",
    "\n",
    "        gru_1, _ = self.gru1(attn1_out)\n",
    "        gru_out = self.gru2(gru_1) # (batch, time_window, DIM)\n",
    "        print('gru_out.shape:', gru_out.shape)\n",
    "\n",
    "        attn2_out, _ = self.attn2(gru_out, attention_size=self.AZ)\n",
    "        print('attn2_out.shape:', attn2_out.shape)\n",
    "\n",
    "        x = self.fc_gru(attn2_out)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "SEPCTURAL_SAMPLES = 10\n",
    "FEATURE_DIM = SEPCTURAL_SAMPLES*6*2\n",
    "CONV_LEN = 3\n",
    "CONV_LEN_INTE = 3#4\n",
    "CONV_LEN_LAST = 3#5\n",
    "CONV_NUM = 64 #CONV_NUM = out channels = in channels * conv_num'\n",
    "CONV_MERGE_LEN = 8\n",
    "CONV_MERGE_LEN2 = 6\n",
    "CONV_MERGE_LEN3 = 4\n",
    "CONV_NUM2 = 64\n",
    "INTER_DIM = 120\n",
    "OUT_DIM = 6#len(idDict)\n",
    "WIDE = 20\n",
    "CONV_KEEP_PROB = 0.8\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_ITER_NUM = 30000\n",
    "\n",
    "select = 'a'\n",
    "\n",
    "metaDict = {'a':[119080, 1193], 'b':[116870, 1413], 'c':[116020, 1477]}\n",
    "TRAIN_SIZE = metaDict[select][0]\n",
    "EVAL_DATA_SIZE = metaDict[select][1]\n",
    "EVAL_ITER_NUM = int(math.ceil(EVAL_DATA_SIZE / BATCH_SIZE))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "            )\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, traindata, valdata, epochs, path, num):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001).to(device)\n",
    "\n",
    "    # low_loss = 0\n",
    "    history = dict(train=[], val=[])\n",
    "    t_s = time.time()\n",
    "\n",
    "    for epoch in range(epochs+1):\n",
    "        t_ss = time.time()\n",
    "        model = model.train()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        print(epoch, \"start!\")\n",
    "        for i, data in enumerate(traindata):\n",
    "            _, inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            print(loss.item().shape)\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        history['train'].append(train_loss)\n",
    "        \n",
    "        if epoch % 50 == 49:\n",
    "            model = model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(valdata,0):\n",
    "                    _, inputs, labels = data\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            history['val'].append(val_loss)\n",
    "            print(f'Epoch = {epoch}, val loss = {val_loss}')\n",
    "\n",
    "        t_ee = time.time()\n",
    "        print(f'Epoch = {epoch}, train loss = {train_loss}, time = {t_ee-t_ss}')\n",
    "\n",
    "        # if val_loss < 1.7 and low_loss == 0:\n",
    "        #     low_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), path+'Attn_model'+num)\n",
    "        # if val_loss < low_loss:\n",
    "        #     low_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), path+'Attn_model'+num)\n",
    "\n",
    "    t_e = time.time()\n",
    "    print(f'Finish! time_cost: {t_e-t_s}')\n",
    "\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnSense()\n",
    "model, history = train_model(model, traindata, valdata, epochs, path, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" save model \"\"\"\n",
    "model_2 = model\n",
    "MODEL_PATH = 'Attn_model_final.pth'\n",
    "torch.save(model_2, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = torch.load(\"Attn_model_final.pth\")\n",
    "print(model_2.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(model, testdata):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = torch.tensor([])\n",
    "    all_lab = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testdata:\n",
    "            _, inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds = torch.cat((all_preds, predicted) ,dim=0)\n",
    "            all_lab = torch.cat((all_lab, labels) ,dim=0)\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "    return all_lab, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = evaluation_model(model, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(pd.DataFrame(confusion_matrix(y_true, y_pred), columns=activities, index=activities), annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
